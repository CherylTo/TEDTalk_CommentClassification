{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TedTalk Recommender for Encouraging Discourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_talk = pd.read_json(\"/Users/cherylto/Dropbox/Ryerson Course/Capstone/Data/ted_talks-10-Sep-2012.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This nifty function gets all the values from specific key pairs despite nesting :) :) \n",
    "def find(key, dictionary):\n",
    "    for k, v in dictionary.items():\n",
    "        if k == key:\n",
    "            yield v\n",
    "        elif isinstance(v, dict):\n",
    "            for result in find(key, v):\n",
    "                yield result\n",
    "        elif isinstance(v, list):\n",
    "            for d in v:\n",
    "                for result in find(key, d):\n",
    "                    yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "def preprocess(document):\n",
    "    words = document.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "    filtered_words = ' '.join([w for w in tokens if not w in stopwords])\n",
    "    filtered_words = ''.join([i for i in filtered_words if not i.isdigit()])\n",
    "    lemmas = lemmatizer.lemmatize(filtered_words)\n",
    "    return ''.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gets comment and video id and puts it in a dataframe\n",
    "_id = []\n",
    "zipped_list = []\n",
    "for i in range(len(json_talk['comments'])):   \n",
    "    doc_id = json_talk['id'][i] #gets the ted_talk id\n",
    "    _id.append(doc_id)\n",
    "    zipped = ()\n",
    "    text = []\n",
    "    date = []\n",
    "    for j in range(len(json_talk['comments'][i])):\n",
    "        \n",
    "        #this actually creates list of list of lists\n",
    "        #gets only first comment\n",
    "        dt, txt = list(find('date', json_talk['comments'][i][j]))[0], list(find('text', json_talk['comments'][i][j]))[0]   \n",
    "        text.append(txt)\n",
    "        date.append(dt)\n",
    "        \n",
    "    zipped = (date, text, doc_id)\n",
    "    zipped_list.append(zipped)\n",
    "    \n",
    "df = pd.DataFrame(zipped_list, columns = ['date', 'comments', '_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding transcripts to the dataframe\n",
    "df['transcripts'] = json_talk['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>comments</th>\n",
       "      <th>_id</th>\n",
       "      <th>transcripts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Sep 10 2012, Sep 10 2012, Sep 10 2012, Sep 10...</td>\n",
       "      <td>[Doesn't gunfire produce visual illumination a...</td>\n",
       "      <td>062dd0f773cd5999a09714a371e1f8017163e2a1</td>\n",
       "      <td>The murder happened a little over 21 years ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Jul 25 2012:, Jul 25 2012:, Jul 25 2012:, Jul...</td>\n",
       "      <td>[I would love to know how they solved the prob...</td>\n",
       "      <td>62f6479a5eca39725798b1ee300bd8d5de3a4ae3</td>\n",
       "      <td>As a kid, I was fascinated with all things air...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Aug 9 2012:, Jul 26 2012:, Jul 11 2012:, Jul ...</td>\n",
       "      <td>[Actually, It is simple idea that we use solar...</td>\n",
       "      <td>b35c0cd294cd10748019833cafa625fc33487065</td>\n",
       "      <td>Good evening. We are in this wonderful open-ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Jul 19 2012:, Jul 16 2012:, Jul 14 2012:, Jul...</td>\n",
       "      <td>[I used to do this as a kid all the time, thou...</td>\n",
       "      <td>0fa6bca242ccb96697e8de570882c6b38746591a</td>\n",
       "      <td>So, last month, the Encyclopaedia Britannica a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Sep 10 2012, 2012-09-07, Aug 29 2012:, Aug 18...</td>\n",
       "      <td>[Where is the video where this guy shows us th...</td>\n",
       "      <td>41db62481aeb978fd13f591755b596ff0616be70</td>\n",
       "      <td>So a few weeks ago, a friend of mine gave this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                date  \\\n",
       "0  [Sep 10 2012, Sep 10 2012, Sep 10 2012, Sep 10...   \n",
       "1  [Jul 25 2012:, Jul 25 2012:, Jul 25 2012:, Jul...   \n",
       "2  [Aug 9 2012:, Jul 26 2012:, Jul 11 2012:, Jul ...   \n",
       "3  [Jul 19 2012:, Jul 16 2012:, Jul 14 2012:, Jul...   \n",
       "4  [Sep 10 2012, 2012-09-07, Aug 29 2012:, Aug 18...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  [Doesn't gunfire produce visual illumination a...   \n",
       "1  [I would love to know how they solved the prob...   \n",
       "2  [Actually, It is simple idea that we use solar...   \n",
       "3  [I used to do this as a kid all the time, thou...   \n",
       "4  [Where is the video where this guy shows us th...   \n",
       "\n",
       "                                        _id  \\\n",
       "0  062dd0f773cd5999a09714a371e1f8017163e2a1   \n",
       "1  62f6479a5eca39725798b1ee300bd8d5de3a4ae3   \n",
       "2  b35c0cd294cd10748019833cafa625fc33487065   \n",
       "3  0fa6bca242ccb96697e8de570882c6b38746591a   \n",
       "4  41db62481aeb978fd13f591755b596ff0616be70   \n",
       "\n",
       "                                         transcripts  \n",
       "0  The murder happened a little over 21 years ago...  \n",
       "1  As a kid, I was fascinated with all things air...  \n",
       "2  Good evening. We are in this wonderful open-ai...  \n",
       "3  So, last month, the Encyclopaedia Britannica a...  \n",
       "4  So a few weeks ago, a friend of mine gave this...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gensim's LSI to find simlarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comments is a list of strings\n",
    "#transcript is string\n",
    "def doc_vec_sims(comments, transcript):\n",
    "    similarity = []\n",
    "    texts = []\n",
    "\n",
    "    #preprocess the text data and build the dictionary\n",
    "    documents = comments\n",
    "    texts = [preprocess(document).split() for document in documents]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    #builds the corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    #Market Matrix Format\n",
    "    corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "    corpus = corpora.MmCorpus('/tmp/corpus.mm')\n",
    "\n",
    "    #comparison document\n",
    "    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics = 10)\n",
    "    query_doc = preprocess(transcript)\n",
    "    vec_bow = dictionary.doc2bow(query_doc.split())\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    #print(vec_lsi)\n",
    "\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    sims = index[vec_lsi]\n",
    "    similarity.append(sims)\n",
    "    return similarity\n",
    "    del dictionary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Questions starters; another way to identify a question is the use of \"?\"\n",
    "wh_ = [\"who's\", 'who', 'where', 'how', 'why', 'tell me', 'explain', \"isn't\", \"is\", \"doesn't\", \"wouldn't\", \"shouldn't\", \"couldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_questions(comments):\n",
    "    #sent_tokenizes each comment, and checks to see if it start with a question word or ends with a question mark\n",
    "    #each comment is labelled, each sentence in the each comment is also labelled\n",
    "    #there are 2 labeling systems here\n",
    "    line = 0\n",
    "    big_list = []\n",
    "    for k, v in enumerate(comments): \n",
    "        #k is comment index\n",
    "        #v is comment\n",
    "        #line is comment idex \n",
    "        sents = sent_tokenize(v)\n",
    "        line += 1\n",
    "        for i in sents:\n",
    "            try:\n",
    "                if i.partition(' ')[0].lower() in wh_ or v.partition(' ')[2].split()[0].lower() in wh_ or i.split()[-1][-1] == '?':\n",
    "                    var = (1, k, i)\n",
    "                elif i.partition(' ')[0].lower() == 'when' and i.split()[-1] == '?':\n",
    "                    var = (1,k, i)\n",
    "                elif i.partition(' ')[0].lower() == 'what' and i.split()[-1] == '?':\n",
    "                    var = (1,k, i)\n",
    "                elif i.partition(' ')[0].lower() == \"can't\" and i.split()[-1] == '?':\n",
    "                    var = (1,k, i)           \n",
    "                else:\n",
    "                    var = (0, k, i)\n",
    "\n",
    "            except:\n",
    "                var = (0, k, i)\n",
    "            big_list.append(var)\n",
    "    return big_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_date(date_list):\n",
    "    date = []\n",
    "    for i in range(len(date_list)):\n",
    "        da = date_list[i].replace(\":\",\"\").replace(\"-\",\" \")\n",
    "        date.append(da)\n",
    "\n",
    "    date2 = []\n",
    "    for i in date:\n",
    "        try: \n",
    "            da = datetime.strptime(i, '%b %d %Y')\n",
    "            date2.append(da)\n",
    "        except:\n",
    "            da = datetime.strptime(i, '%Y %m %d')\n",
    "            date2.append(da)\n",
    "            \n",
    "    days_posted=[]\n",
    "    for i in date2:\n",
    "        da = most_recent - i\n",
    "        days_posted.append(da.days)\n",
    "    return days_posted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritizing TedTalk Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_recent = datetime.strptime('Sep 20 2012', '%b %d %Y')\n",
    "def prioritize_comments(doc_index):\n",
    "\n",
    "    comments = df.iloc[doc_index]['comments']\n",
    "    transcripts = df['transcripts'][doc_index]\n",
    "    d_list = df.iloc[doc_index]['date']\n",
    "    \n",
    "    similarity = doc_vec_sims(comments, transcripts)\n",
    "    sims = similarity[0]\n",
    "    \n",
    "    comm_ = [c for c in comments]\n",
    "    \n",
    "    #Find Questions\n",
    "    big_list = find_questions(comments)\n",
    "    question_df = pd.DataFrame(big_list, columns = ['TF', 'comment_id', 'comment'])\t\n",
    "    tmp = question_df.groupby('comment_id').sum()\n",
    "    \n",
    "    ##True/False\n",
    "    question = []\n",
    "    for i in tmp.values:\n",
    "        if i >= 1:\n",
    "            question.append(1)\n",
    "        else:\n",
    "            question.append(0)\n",
    "    \n",
    "    ##Comment Length\n",
    "    comment_length = [len(i) for i in comments]\n",
    "    \n",
    "    ##Date\n",
    "    d_posted = clean_date(d_list)\n",
    "\n",
    "    #create dataframe for sorting\n",
    "    tmp = pd.DataFrame({\"date_posted\": d_list, \"days_posted\": d_posted, \"similarity\": sims, \"question\": question, \"comment length\": comment_length, \"comments\": comm_})\n",
    "    \n",
    "    #top 10 items according to definition\n",
    "    ind = tmp.sort_values(['days_posted', 'similarity', 'question', 'comment length'], ascending = [True, False, False, False])[:10].index\n",
    "    \n",
    "    #print recommendations\n",
    "    li = ind.values\n",
    "    for i in li:\n",
    "        print(\"document number: \"+str(i), \n",
    "              \"days posted: \"+str(tmp.iloc[i]['days_posted']), \n",
    "              \"similarity score: \"+str(tmp.iloc[i]['similarity']),\n",
    "              \"comment: \"+str(tmp.iloc[i]['comments']), \n",
    "               sep=\"\\n\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document number: 0\n",
      "days posted: 50\n",
      "similarity score: 0.4660036\n",
      "comment: Although the soldiers' plight is undoubtedly real and any suffering is worthy of our sympathy, this piece is undoubtedly propaganda and doesn't belong in TED.\n",
      "\n",
      "document number: 1\n",
      "days posted: 57\n",
      "similarity score: 0.44467694\n",
      "comment: Thank you for showing sympathy and humanity to the soldiers.\n",
      "\n",
      "document number: 2\n",
      "days posted: 114\n",
      "similarity score: 0.6133998\n",
      "comment: if we could only put the camera on the minds of the u.s. soldiers themselves... the suicide death toll has now exceeded the combat death toll for these folks. horrific numbers to digest. http://www.psychalive.org/2012/05/memorial-day-an-opportunity-to-reach-out-to-veterans/ \n",
      "what the hell do we do about that?\n",
      "\n",
      "document number: 3\n",
      "days posted: 122\n",
      "similarity score: 0.095796265\n",
      "comment: Crocodile tears for imperialist invaders. Hardly an Idea Worth Spreading Chris Anderson.\n",
      "\n",
      "document number: 4\n",
      "days posted: 167\n",
      "similarity score: 0.18614173\n",
      "comment: I am applying to speak at TED 2013 about my efforts to build upon Jeremy Gilley's International Peace One Day UN Resolution (A Res 55/282). I have actually proposed to the United Nations that an International Peace EVERY Day Treaty For Global Truce and Global Cease Fire be submitted to all nations for them to review and sign. I call for all 192 world leaders to come together in the United Nations on 9/21/12 to sign the treaty. They are invited to raise their right hand and in unison say \"Global Truce Starts Now! Global Cease Fire Starts Now! May Peace Prevail on Earth and Let It Begin With Me! Then all 192 world leaders are invited to simultaneously issue cease fire orders to their military commanders. I am inviting religious, educational and informal leaders to take part also! \n",
      "I have already written to many world leaders and submitted my idea to them. I have submitted this proposal to the UN Secretary General. I have requested to meet him and asked if I may speak about this idea to the entire UN General Assembly and the entire UN Security Council.  \n",
      "I am inviting all humanity to help make this idea viral. I believe we can create peace. I had to imagine peace and conceptualize backward from the premise that peace is possible. I then reverse engineered. I asked myself what would we need to do to get a global truce and a global cease fire put into place and adhered to.  \n",
      "I wrote up a 200 page proposal containing over 500 ideas on  www.goingforaglobaltruce.com  This is a never ending project that we have to work on 24/7/365. It will take all of humanity's brilliance to figure out the solution to this problem. It won't be easy but nothing worth while is.  \n",
      "I challenge humanity to end all war!  Please make a nonviolent plan as to how you personally can help increase peace on Earth and then please implement that plan.  Please help spread the idea of a global truce and a global cease fire around the world tol the general public, the media and political decision makers.\n",
      "\n",
      "document number: 5\n",
      "days posted: 341\n",
      "similarity score: 0.65377\n",
      "comment: As an Iraqi, I registered a username just to thank all the people who have called out on this horrible propaganda fluff. Joseph Goebbels would be so proud. I find it hilarious that the shoddy film-maker (who's pulling a Kathryn Bigelow, and doing a lousy job at it) is actually trying to tackle the commentators for their lack of 'sensitivity' towards the troops. You'd think from her wailing that American Marines are conscripts sent to a mission against their will. Oh, please. Stop the hyperbole, you're embarrassing yourself.\n",
      "\n",
      "document number: 6\n",
      "days posted: 355\n",
      "similarity score: 0.27979773\n",
      "comment: Another film for the hypothetical TED playhouse. We need access to these films!\n",
      "\n",
      "document number: 7\n",
      "days posted: 417\n",
      "similarity score: 0.32497805\n",
      "comment: Ahmed, \n",
      "I was a soldier in Iraq and part of the unit represented in this documentary.  I would question anyone that told you we went over there to kill Iraqi people.  I spent the better part of my time in Iraq protecting the Iraqi people from insurgents who came from countries outside of Iraq to kill Iraqi people.   \n",
      "We protected families men, women, and children from being robbed and murdered.  We treated Iraqi civilians who were blown up by indiscriminate car bombings and IED's. \n",
      "One insurgent that we captured was a school teacher from another country.  He was told he needed to go to Iraq and fight the Americans because we were \"Eating the Iraqi Children\".  He actually believed we were cannibals feeding on the people of Iraq. \n",
      "We Americans have a history of questioning our leaders and politicians.  Please do the same and actually learn the truth before blindly acting on another's false statements. \n",
      "\"millions of iraq people were killed,no body care and that hurts me\" \n",
      "I can guarantee you that this statement is false.  I think about many of the Iraqi people daily.  Some I will never forget.\n",
      "\n",
      "document number: 8\n",
      "days posted: 418\n",
      "similarity score: 0.4776335\n",
      "comment: sometimes i feel that US soldiers are victims of US policy  \n",
      "they go to wars with bad ideas in their heads they thought that they do that for peace and for the good of the world \n",
      "sometimes I don't blame them they have families they know what is the feeling when you lose some body \n",
      "Whole blame is on the government. \n",
      "millions of iraq people were killed,no body care and that hurts me \n",
      "MS,Deborah  i don't blame you, you are an american  but I want you to think about iraq people what is thier fault?? to be killed like that with no mercy! \n",
      "iam egyptian and have mixed feelings towards  US soldiers ,i find them killers or men who forced to kill in the name of freedom indirectly but of course i hate the US government so much  \n",
      "i just ask you to think about millions of iraq people that died or became homeless,fatherless,with destroyed country , just feel what others feel and don't think only for yourself \n",
      "I DON'T BLAME YOU ,it was a good talk by showing  the human touch \n",
      " i ask you to complete your message by showing to the people the view of the other side of the conflict  \n",
      "as a new war tape. \n",
      "sorry for my english\n",
      "\n",
      "document number: 9\n",
      "days posted: 463\n",
      "similarity score: 0.5660197\n",
      "comment: i just want to say......... plesae stop the war!! violence is not a solution, die and kill are not solutions. i feell sorry about every parts involved, no matter if you are an iraqui or an american soldier, its tragic, please dont support the war, any one at all, dont support the interests of a few with power that dont care about us, they really REALLY DONT CARE.  http://is.gd/esUqaw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prioritize_comments(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This one removes the sort criteria for date, and prioritizes question in the search criteria.\n",
    "most_recent = datetime.strptime('Sep 20 2012', '%b %d %Y')\n",
    "def pc_model2(doc_index):\n",
    "\n",
    "    comments = df.iloc[doc_index]['comments']\n",
    "    transcripts = df['transcripts'][doc_index]\n",
    "    d_list = df.iloc[doc_index]['date']\n",
    "    \n",
    "    similarity = doc_vec_sims(comments, transcripts)\n",
    "    sims = similarity[0]\n",
    "    \n",
    "    comm_ = [c for c in comments]\n",
    "    \n",
    "    #Find Questions\n",
    "    big_list = find_questions(comments)\n",
    "    question_df = pd.DataFrame(big_list, columns = ['TF', 'comment_id', 'comment'])\t\n",
    "    tmp = question_df.groupby('comment_id').sum()\n",
    "    \n",
    "    ##True/False\n",
    "    question = []\n",
    "    for i in tmp.values:\n",
    "        if i >= 1:\n",
    "            question.append(1)\n",
    "        else:\n",
    "            question.append(0)\n",
    "    \n",
    "    ##Comment Length\n",
    "    comment_length = [len(i) for i in comments]\n",
    "    \n",
    "    ##Date\n",
    "    d_posted = clean_date(d_list)\n",
    "\n",
    "    #create dataframe for sorting\n",
    "    tmp = pd.DataFrame({\"date_posted\": d_list, \"days_posted\": d_posted, \"similarity\": sims, \"question\": question, \"comment length\": comment_length, \"comments\": comm_})\n",
    "    \n",
    "    #top 10 items according to definition\n",
    "    ind = tmp.sort_values(['question', 'similarity', 'comment length'], ascending = [False, False, False])[:10].index\n",
    "    \n",
    "    #print recommendations\n",
    "    li = ind.values\n",
    "    for i in li:\n",
    "        print(\"document number: \"+str(i),\n",
    "              \"days posted: \"+str(tmp.iloc[i]['days_posted']), \n",
    "              \"similarity score: \"+str(tmp.iloc[i]['similarity']), \n",
    "              \"comment: \"+str(tmp.iloc[i]['comments']),\n",
    "             sep='\\n')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document number: 53\n",
      "days posted: 1075\n",
      "similarity score: 0.9377416\n",
      "comment: Malcolm Gladwell covers this in Tipping Point, that you should market to \"Mavens,\" people who are interested in specific products and get excited about it with others.  People like \"remarkable\" things?  Didn't Piaget cover this 60 years ago with the children's tendency to look at \"novel\" things?  I like his other talk about Tribes better.\n",
      "\n",
      "document number: 104\n",
      "days posted: 1985\n",
      "similarity score: 0.8754969\n",
      "comment: If you have anything to sell, watch this video.  \r\n",
      "Seth gets you thinking, as usual. Is what you are selling remarkable? Do you or your product/idea have otaku? Are you focusing on the people who are like you? Who care about what you have to say?  \r\n",
      "\"Favorite quote from this video: \"Oh, you like my ring? It's my grandmother!\" Sorry, you gotta watch it if you want to know.\n",
      "\n",
      "document number: 52\n",
      "days posted: 1067\n",
      "similarity score: 0.7711279\n",
      "comment: There are many remarkable products and services that have been out on the marketplace, but many just never had the following, because very few people who would have been excited about it and would have told their colleagues, family and friends.  These products died, not because they weren't great, but more than likely, the right people didn't know they existed.  I loved the idea about \"me mail\".  But really isn't that what Google is trying to accomplish, to understand your every little want and desire, so as they can get the 'right' email to you, at the 'right' time, with the 'best' price, and at the 'most' convenient location?  We all run around kicking and screaming about our privacy, but in the end, receiving information that is targeted to my likes and desires is very desirable.  In the end, I am just a geek and I want to have the latest and greatest, and you bet that I am going to tell everyone I know, whilst shouting from the highest social networking medium!\n",
      "\n",
      "document number: 56\n",
      "days posted: 1156\n",
      "similarity score: 0.7534784\n",
      "comment: \"Pearl Jam 96 albums released in the last two years, everyone made a profit\" \n",
      "What is he talking about!? \n",
      "96 ALBUMS IN TWO YEARS \n",
      "even if he meant tracks he would be completely wrong!\n",
      "\n",
      "document number: 32\n",
      "days posted: 752\n",
      "similarity score: 0.7449301\n",
      "comment: Hmmm, i wonder how that works with making the dead into diamonds. Even more so, could you get something like that appraised?\n",
      "\n",
      "document number: 0\n",
      "days posted: 29\n",
      "similarity score: 0.6844717\n",
      "comment: Nice lecture, although I did not understand why he said that being very good is bad... Anyone wishes to try and explain this point to me?\n",
      "\n",
      "document number: 83\n",
      "days posted: 1704\n",
      "similarity score: 0.6461935\n",
      "comment: ok, so I should ask you guys,  \n",
      "is the idea for a good design for no money a pink enough cow for you? \n",
      "and what should I do to make you click and look at my profile? :) \n",
      "ps: I should admit - it is the bold head of Seth, that made me buy his book.\n",
      "\n",
      "document number: 43\n",
      "days posted: 986\n",
      "similarity score: 0.5381498\n",
      "comment: Definition of remarkable = worth making a comment about. This talk is remarkable; the video is nearly 7 years old, yet relevant for today. The market may be changing rapidly but some things, core ideas, values, and ethics remain constant. The question bugs me about being remarkable just for being remarkable ... if the core of the our business stays the same, but the way we have to reinvent it keeps changing, are we drawn into a cycle of invention that does not move business forward but just tries to keep us in front of a new crowd? Do we spend money on R &amp; D or a Lava Lamp to shout - still here, same town, same lake, but look a giant Lava lamp! This is a whole new world for me as I try to develop a innovative online project for the Russian market, loads to learn, a few failures to make, great video for fresh ideas. Like the sliced bread joke ... when a rocket scientist explains something to a colleague who does not get it, what do they say ... it is not xxxxxx science!\n",
      "\n",
      "document number: 79\n",
      "days posted: 1620\n",
      "similarity score: 0.51164335\n",
      "comment: Good entertainment and one valid answer on how to (sometimes) win at marketing. Seems a good model for challenger brands, where disruption is a strong tactic, or niche products, where you just need to be truly loved by a few people, and repelling the mainstream is OK. In both cases, though, perhaps it is a strategy of winning the battle but losing the war \n",
      "What of the boring, rational, non-impulsive mainstream, however? Does the world's most successful car company, Toyota, have any purple cows? Sometimes being quietly brilliant at what you do, and trusting the good judgment of ordinary people can work too. Not a recipe worth documenting for coffee table book success, however. Style is more entertaining than substance\n",
      "\n",
      "document number: 68\n",
      "days posted: 1306\n",
      "similarity score: 0.48991537\n",
      "comment: This is what TED is all about;  big picture thinking and revealing trends that we experience every day but don't apprehend. Inspiring and worth remarking about to others.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pc_model2(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
